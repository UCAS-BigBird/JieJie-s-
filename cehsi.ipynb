{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--tree_method TREE_METHOD]\n",
      "                             [--sparsity SPARSITY] [--rows ROWS]\n",
      "                             [--columns COLUMNS] [--iterations ITERATIONS]\n",
      "                             [--test_size TEST_SIZE] [--params PARAMS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\chengben\\AppData\\Roaming\\jupyter\\runtime\\kernel-102a6bd4-53bc-4e3b-bb7b-2ffd250b310f.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chengben\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "RNG = np.random.RandomState(1994)\n",
    "\n",
    "\n",
    "def run_benchmark(args):\n",
    "    \"\"\"Runs the benchmark.\"\"\"\n",
    "    try:\n",
    "        dtest = xgb.DMatrix('dtest.dm')\n",
    "        dtrain = xgb.DMatrix('dtrain.dm')\n",
    "\n",
    "        if not (dtest.num_col() == args.columns\n",
    "                and dtrain.num_col() == args.columns):\n",
    "            raise ValueError(\"Wrong cols\")\n",
    "        if not (dtest.num_row() == args.rows * args.test_size\n",
    "                and dtrain.num_row() == args.rows * (1 - args.test_size)):\n",
    "            raise ValueError(\"Wrong rows\")\n",
    "    except:\n",
    "        print(\"Generating dataset: {} rows * {} columns\".format(args.rows, args.columns))\n",
    "        print(\"{}/{} test/train split\".format(args.test_size, 1.0 - args.test_size))\n",
    "        tmp = time.time()\n",
    "        X = RNG.rand(args.rows, args.columns)\n",
    "        y = RNG.randint(0, 2, args.rows)\n",
    "        if 0.0 < args.sparsity < 1.0:\n",
    "            X = np.array([[np.nan if RNG.uniform(0, 1) < args.sparsity else x for x in x_row]\n",
    "                          for x_row in X])\n",
    "\n",
    "        train_rows = int(args.rows * (1.0 - args.test_size))\n",
    "        test_rows = int(args.rows * args.test_size)\n",
    "        X_train = X[:train_rows, :]\n",
    "        X_test = X[-test_rows:, :]\n",
    "        y_train = y[:train_rows]\n",
    "        y_test = y[-test_rows:]\n",
    "        print(\"Generate Time: %s seconds\" % (str(time.time() - tmp)))\n",
    "        del X, y\n",
    "\n",
    "        tmp = time.time()\n",
    "        print(\"DMatrix Start\")\n",
    "        dtrain = xgb.DMatrix(X_train, y_train, nthread=-1)\n",
    "        dtest = xgb.DMatrix(X_test, y_test, nthread=-1)\n",
    "        print(\"DMatrix Time: %s seconds\" % (str(time.time() - tmp)))\n",
    "        del X_train, y_train, X_test, y_test\n",
    "\n",
    "        dtest.save_binary('dtest.dm')\n",
    "        dtrain.save_binary('dtrain.dm')\n",
    "\n",
    "    param = {'objective': 'binary:logistic'}\n",
    "    if args.params != '':\n",
    "        param.update(ast.literal_eval(args.params))\n",
    "\n",
    "    param['tree_method'] = 'gpu_hist'\n",
    "    print(\"Training with '%s'\" % param['tree_method'])\n",
    "    tmp = time.time()\n",
    "    xgb.train(param, dtrain, args.iterations, evals=[(dtest, \"test\")])\n",
    "    print(\"Train Time: %s seconds\" % (str(time.time() - tmp)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"The main function.\n",
    "\n",
    "    Defines and parses command line arguments and calls the benchmark.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--tree_method', default='gpu_hist')\n",
    "    parser.add_argument('--sparsity', type=float, default=0.0)\n",
    "    parser.add_argument('--rows', type=int, default=1000000)\n",
    "    parser.add_argument('--columns', type=int, default=50)\n",
    "    parser.add_argument('--iterations', type=int, default=500)\n",
    "    parser.add_argument('--test_size', type=float, default=0.25)\n",
    "    parser.add_argument('--params', default='',\n",
    "                        help='Provide additional parameters as a Python dict string, e.g. --params '\n",
    "                             '\\\"{\\'max_depth\\':2}\\\"')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    run_benchmark(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Sep 10 16:42:50 2019\n",
    "\n",
    "@author: chengben\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int': #前三个字符为int那就为整型\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:#先看八位整型能不能满足条件\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:#不是整型就是浮点型\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def logloss(y_true, y_pred,deta = 3, eps=1e-15):\n",
    "    # Prepare numpy array data\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    assert (len(y_true) and len(y_true) == len(y_pred))\n",
    "    # Clip y_pred between eps and 1-eps\n",
    "    p = np.clip(y_pred, eps, 1-eps)\n",
    "    loss = np.sum(- y_true * np.log(p) * deta - (1 - y_true) * np.log(1-p))\n",
    "    return loss / len(y_true)\n",
    "\n",
    "train = reduce_mem_usage(pd.read_csv('round1_diac2019_train.csv'))\n",
    "\n",
    "#drop_duplicates() 去除该列的重复项\n",
    "all_customer = pd.DataFrame(train[['customer_id']]).drop_duplicates(['customer_id']).dropna()\n",
    "#print(all_customer.shape)\n",
    "#\n",
    "#train['order_pay_time'] = pd.to_datetime(train['order_pay_time'])\n",
    "#train['order_pay_date'] = train['order_pay_time'].dt.date\n",
    "#\n",
    "#train_data = train[train['order_pay_date'].astype(str)<'2013-07-03']\n",
    "#online_train_data = train\n",
    "#train_labels = train[train['order_pay_date'].astype(str)>='2013-07-03'] \n",
    "#\n",
    "##简单的特征生成代码，改进空间很大\n",
    "#\n",
    "#def make_feature_and_label(date1,date2,isSubmit):\n",
    "#    date1['count'] = 1   \n",
    "#    #每个id的购物数量   agg({}) 字典里面的key如果在dataframe里面那么['count']可以省略\n",
    "#    #被分组后的数据也是dataframe类型\n",
    "#    #如customer_id  列索引第一个是分组依据：customer_id 第二个是 count\n",
    "#    customer_id = date1.groupby(['customer_id'], as_index=False)['count'].agg({'count':'count'}) #对count进行count聚合\n",
    "#    #统计每个用户买的商品的价格\n",
    "#    #行索引由于是as_index=False 所以还是数字， 但是列索引分别为： customers_id good_price_max,good_price_mean,good_price_min\n",
    "#    good_price = date1.groupby(['customer_id'], as_index=False)['goods_price'].agg({'goods_price_max':'max',\n",
    "#                                                                                     'goode_price_mean':'mean',\n",
    "#                                                                                     'goode_price_max':'min'})\n",
    "#    \n",
    "#    last_time = date1.groupby(['customer_id'], as_index=False)['order_pay_date'].agg({'order_pay_date_last':'max','order_pay_date_first':'min'})\n",
    "#    \n",
    "#    #将以上特征合并到一个表格，但是由于他们都是dataframe 所以用merge 合并键为customer_id 合并方式以左侧为准\n",
    "#    data = pd.merge(customer_id,good_price,on=['customer_id'],how='left',copy=False)\n",
    "#    data = pd.merge(data, last_time, on=['customer_id'],how='left',copy=False)\n",
    "#    data['long_time'] = pd.to_datetime(data['order_pay_date_last']) - pd.to_datetime(data['order_pay_date_first'])\n",
    "#    data['long_time'] = data['long_time'].dt.days + 1\n",
    "#    del data['order_pay_date_first']\n",
    "#    if isSubmit==False: #表示为训练集不仅要提取特征，还要进行标注，利用date2进行标注\n",
    "#        data['order_pay_date_last'] = pd.to_datetime(date2['order_pay_date'].min()) - pd.to_datetime(data['order_pay_date_last'])\n",
    "#        data['order_pay_date_last'] = data['order_pay_date_last'].dt.days + 1\n",
    "#        data['labels'] = 0\n",
    "#        data.loc[data['customer_id'].isin(list(date2['customer_id'].unique())),'labels'] = 1\n",
    "#        print(data['labels'].mean())\n",
    "#    else:#表示为线上数据集，只需要进行特征提取就行\n",
    "#        data['order_pay_date_last'] = pd.to_datetime('2013-12-31')- pd.to_datetime(data['order_pay_date_last'])\n",
    "#        data['order_pay_date_last'] = data['order_pay_date_last'].dt.days + 1\n",
    "#    print(data.shape)\n",
    "#    return data\n",
    "#train = make_feature_and_label(train_data,train_labels,False)\n",
    "#train.to_csv('data/train_data.csv',index=False)\n",
    "#submit = make_feature_and_label(online_train_data,None,True)\n",
    "## ============================获取用户行为特征=========================\n",
    "##def user_item(data_date):\n",
    "##    \"\"\"\n",
    "##    返回用户在data_date时间段内购买的商品数，返回形式\n",
    "##     DataFrame = {index = ‘customer_id’\n",
    "##                 columns = '(hours,behavours)'\n",
    "##                 values = 'user_id in hour behavour counts'\n",
    "##                 }\n",
    "##    \"\"\"\n",
    "##    #透视表用来统计频率，数据变为index和columns\n",
    "##    # pd.crosstab([df.columns_1,df.columns_2],df.columns_3,dropna=False) 数据交叉表用于统计在索引为df.columns_1 和df.columns_2的条件下，统计df.columns_3的数值\n",
    "##    # 返回一个crosstab对象，使用函数可以返回一个DataFrame对象\n",
    "##    # 使用df.unstack(fill_value = 0)可以将第二级行索引转换为由上至下的最下面的列索引\n",
    "##    #交叉表在于一个交叉二字，以两个参数为例，df.columns_1,df.coulmns_3 df.columns_1 为表格的index，df.columns_2为表格的columns ,二者之间对应的表格\n",
    "##    #表示二者发生关系的次数。比如用户1和8的交集为6，表明用户1，在8这个时间段这几天内有过6次操作，如果再细分一点是什么操作，还可以如本程序所示，多加一个参数\n",
    "##    user_act_count = pd.crosstab([beforesomeday.user_id,beforesomeday.behavior_type],beforesomeday.hours,dropna=False)\n",
    "##    user_act_count = user_act_count.unstack(fill_value = 0)\n",
    "##    return user_act_count\n",
    "#\n",
    "#param = {\n",
    "#    'num_leaves':128,\n",
    "#    'objective':'binary',\n",
    "#    'max_depth':-1,\n",
    "#    'learning_rate':0.1,\n",
    "#    'metric':'binary_logloss'}\n",
    "# #划分数据集   \n",
    "#y = train.pop('labels')  \n",
    "#\n",
    "#feature = [x for x in train.columns if x not in ['customer_id']]\n",
    "#X = train[feature]  \n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42,stratify=y)    \n",
    "\n",
    "#训练模型\n",
    "#trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "#val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "#lgbm = lgb.train(param,trn_data,valid_sets=[trn_data,val_data],num_boost_round = 10000 ,early_stopping_rounds=25,verbose_eval=50) \n",
    "from lightgbm.sklearn import LGBMRegressor #d导入的是sklearn版本，原生版本略有不同   \n",
    "#model = LGBMRegressor(n_jobs=-1,learning_rate=0.05,\n",
    "#                    max_depth=16,\n",
    "#                     n_estimators=10,\n",
    "#                     num_leaves=30,\n",
    "#                     reg_alpha=1.5,\n",
    "#                     reg_lambda=0.7,\n",
    "#                     min_child_samples=27,\n",
    "#                     min_split_gain=0.1,\n",
    "#                     colsample_bytree=0.2)\n",
    "model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "       colsample_bytree=0.833, gamma=0.11, learning_rate=0.005, max_delta_step=0,\n",
    "       max_depth=25, min_child_weight=1, missing=None, n_estimators=200,\n",
    "       n_jobs=-1, nthread=50, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=1.7, reg_lambda=0.6, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=0.9)\n",
    "train = pd.read_csv('train_x_add1.csv')\n",
    "label = LabelEncoder()\n",
    "train['order_pay_date_last'] = pd.to_datetime('2013-12-31') - pd.to_datetime(train['order_pay_date_last'])\n",
    "train['order_pay_date_last'] = train['order_pay_date_last'].dt.days + 1\n",
    "train['order_pay_date_first'] =  pd.to_datetime(train['order_pay_date_last']) - pd.to_datetime('2013-7-4') \n",
    "train['order_pay_date_first'] = train['order_pay_date_first'].dt.days + 1\n",
    "train['customer_province_code'] = label.fit_transform(train['customer_province'])\n",
    "train['customer_city_code'] = label.fit_transform(train['customer_city'])\n",
    "Y = train.pop('labels')\n",
    "X = train.drop(['customer_id','goods_id','customer_province','customer_city'],axis=1).fillna(0)\n",
    "model.fit(X,Y)\n",
    "\n",
    "\n",
    "#进行预测  \n",
    "test = pd.read_csv('test_x_add1.csv')\n",
    "test['order_pay_date_last'] = pd.to_datetime('2013-12-31') - pd.to_datetime(test['order_pay_date_last'])\n",
    "test['order_pay_date_last'] = test['order_pay_date_last'].dt.days + 1\n",
    "test['order_pay_date_first'] =  pd.to_datetime(test['order_pay_date_last']) - pd.to_datetime('2013-7-4') \n",
    "test['order_pay_date_first'] = test['order_pay_date_first'].dt.days + 1\n",
    "test['customer_province_code'] = label.fit_transform(test['customer_province'])\n",
    "test['customer_city_code'] = label.fit_transform(test['customer_city'])\n",
    "\n",
    "X_submit = test.drop(['customer_id','goods_id','customer_province','customer_city'],axis=1).fillna(0)\n",
    "y_submit = model.predict(X_submit)\n",
    "\n",
    "#提交的结果\n",
    "submit_df = test[['customer_id']]\n",
    "submit_df['result'] = y_submit    \n",
    "all_customer = pd.merge(all_customer,submit_df,on=['customer_id'],how='left',copy=False)\n",
    "all_customer = all_customer.sort_values(['customer_id'])#默认升序 ascending=True False降序\n",
    "all_customer['customer_id'] = all_customer['customer_id'].astype('int64') \n",
    "all_customer['result'] = all_customer['result'].fillna(0)\n",
    "all_customer['result'] = all_customer['result'].apply(lambda x: 1 if x > 1.0 else x)\n",
    "all_customer['result'] = all_customer['result'].apply(lambda x: 0 if x <0 else x)\n",
    "all_customer.to_csv('mpdf_baseline111.csv',index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep 16 12:31:09 2019\n",
    "\n",
    "@author: chengben\n",
    "\"\"\"\n",
    "#LGBMRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def logloss(y_true, y_pred,deta = 3.5, eps=1e-15):\n",
    "    # Prepare numpy array data\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    assert (len(y_true) and len(y_true) == len(y_pred))\n",
    "    # Clip y_pred between eps and 1-eps\n",
    "    p = np.clip(y_pred, eps, 1-eps)\n",
    "    loss = np.sum(- y_true * np.log(p) * deta - (1 - y_true) * np.log(1-p))\n",
    "    return loss / len(y_true)\n",
    "\n",
    "train = pd.read_csv('train_x_add.csv')\n",
    "#test = pd.read_csv('test_x_ziji.csv')\n",
    "trian = train.sample(frac=1)\n",
    "label = LabelEncoder()\n",
    "train['order_pay_date_last'] = pd.to_datetime('2013-12-31') - pd.to_datetime(train['order_pay_date_last'])\n",
    "train['order_pay_date_last'] = train['order_pay_date_last'].dt.days + 1\n",
    "train['order_pay_date_first'] =  pd.to_datetime(train['order_pay_date_last']) - pd.to_datetime('2013-7-4') \n",
    "train['order_pay_date_first'] = train['order_pay_date_first'].dt.days + 1\n",
    "train['customer_province'] = label.fit_transform(train['customer_province'])\n",
    "train['customer_city_code'] = label.fit_transform(train['customer_city'])\n",
    "Y = train.pop('labels')\n",
    "X = train.drop(['customer_id','goods_id','customer_province','customer_city','order_pay_date_last','order_pay_date_first'],axis=1).fillna(0)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm.sklearn import LGBMRegressor #d导入的是sklearn版本，原生版本略有不同\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle = False, random_state=12)\n",
    "\n",
    "model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.1,\n",
    "       colsample_bytree=0.733, gamma=0.11, learning_rate=0.05, max_delta_step=0,\n",
    "       max_depth=25, min_child_weight=1, missing=None, n_estimators=20,\n",
    "       n_jobs=-1, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=1.7, reg_lambda=0.6, scale_pos_weight=7, seed=None,\n",
    "       silent=True, subsample=1,gpu_id = 0, max_bin = 16, tree_method = 'gpu_hist')\n",
    "\n",
    "# def myloss(real,pre):\n",
    "#     grad = -(real/pre + (1-real)/(1-pre))\n",
    "#     hess = real/pre**2 +(1-real)/(1-pre)**2\n",
    "#     return grad,hess\n",
    "# mse1 = []\n",
    "# mse2 = []\n",
    "for train,val in kfold.split(X): #train_test_split留出法\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = Y.iloc[train]\n",
    "    X_val = X.iloc[val]\n",
    "    y_val = Y.iloc[val]\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred1 = model.predict(X_val)\n",
    "    error1 = logloss(y_val,y_pred1)\n",
    "    mse1.append(error1)\n",
    "    y_pred2 = model.predict(X_train)\n",
    "    error2 = logloss(y_train,y_pred2)\n",
    "    mse2.append(error2)\n",
    "print('验证集准确率',np.mean(mse1),mse1)\n",
    "print(\"训练集准确率\",np.mean(mse2),mse2)\n",
    "\n",
    "#param_test = {  'gamma':np.linspace(0.11,0.12,2)\n",
    "#                'max_depth':range(8,16,2),\n",
    "#            'min_child_weight':np.linspace(0.1,0.8,8)\n",
    "#              'num_leaves':range(30,100,5),\n",
    "#         'min_split_gain':np.linspace(0.1,1,10),\n",
    "#     'min_child_samples':range(10,30,1),\n",
    "#     'reg_alpha':np.linspace(1.5,2,6),\n",
    "#            'reg_lambda':np.linspace(0.5,1.2,8) \n",
    "#            }\n",
    "##缩进的是需要调优的，行间隔表示调优分组\n",
    "#model = XGBRegressor(n_estimators=21,\n",
    "#                     \n",
    "#                     max_depth=25,\n",
    "#                     min_child_weight=1,\n",
    "#                     \n",
    "#                     gamma=0.11, #在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。\n",
    "#                     \n",
    "#                     subsample=1.0,\n",
    "#                     colsample_bytree=1,#和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。\n",
    "#                                         \n",
    "#                     colsample_bylevel=1,#用来控制树的每一级的每一次分裂，对列数的采样的占比\n",
    "#                     \n",
    "#                     reg_alpha=1.2,  #L1正则化\n",
    "#                     reg_lambda=0.6, #L2正则化\n",
    "#                     \n",
    "#                     learning_rate=0.05, \n",
    "#                     \n",
    "#                       base_score=0.5,\n",
    "#                       booster='gbtree', \n",
    "#                       max_delta_step=0,\n",
    "#                       missing=None, \n",
    "#                       n_jobs=-1, \n",
    "#                       nthread=50, \n",
    "#                       objective=myloss, \n",
    "#                       random_state=0,\n",
    "#\n",
    "#                       scale_pos_weight=1, \n",
    "#                       seed=None,\n",
    "#                       silent=True, \n",
    "#                       )\n",
    "#gsearch2 = RandomizedSearchCV(model, param_test,iid=False, cv=5,verbose=1,n_jobs=-1,scoring='neg_mean_squared_error',n_iter=20)\n",
    "#gsearch2.fit(X, Y)\n",
    "#print( gsearch2.best_params_, gsearch2.best_score_)\n",
    "\n",
    "\n",
    "#param_test = {'n_estimators':range(20,800,20),\n",
    "#             'min_samples_split':range(5,200,20),\n",
    "#             'min_samples_leaf':range(5,100,10),\n",
    "#             'max_depth':range(2,12,1),\n",
    "#             'max_features':range(2,12,1)\n",
    "#     \n",
    "#}\n",
    "#model = RandomForestRegressor(n_estimators=800,\n",
    "#                                 min_samples_split=40,\n",
    "#                                 min_samples_leaf=20,\n",
    "#                                 max_depth=8,\n",
    "#                                 max_features='sqrt' ,\n",
    "#                                 random_state=10)\n",
    "#gsearch2 = RandomizedSearchCV(model, param_test,iid=False, cv=5,verbose=10,n_jobs=-1,scoring='neg_mean_squared_error',n_iter=400)\n",
    "#gsearch2.fit(X, Y)\n",
    "#print( gsearch2.best_params_, gsearch2.best_score_)\n",
    "\n",
    "#}\n",
    "#max_depth ：设置树深度，深度越大可能过拟合\n",
    "#num_leaves：因为 LightGBM 使用的是 leaf-wise 的算法，因此在调节树的复杂程度时，使用的是 num_leaves 而不是 max_depth。大致换算关系：num_leaves = 2^(max_depth)，但是它的值的设置应该小于 2^(max_depth)，否则可能会导致过拟合。\n",
    "#\n",
    "#作者：慕斯王\n",
    "#链接：https://www.imooc.com/article/43784?block_id=tuijian_wz\n",
    "#来源：慕课网\n",
    "#lgb = LGBMRegressor(n_jobs=-1,learning_rate=0.05,\n",
    "#                    max_depth=16,\n",
    "#                     n_estimators=800,\n",
    "#                     num_leaves=30,\n",
    "#                     reg_alpha=1.5,\n",
    "#                     reg_lambda=0.7,\n",
    "#                     min_child_samples=27,\n",
    "#                     min_split_gain=0.1,\n",
    "#                     colsample_bytree=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model,'ou.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('001.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "model = joblib.load('ou.pkl')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "# sns.set_style('darkgrid')\n",
    " \n",
    "features_list = X_train.columns.values\n",
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[-50:]\n",
    " \n",
    "plt.figure(figsize=(5,7))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature importances')\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://ndownloader.figshare.com/files/5976039\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c1164fdaed89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fetch dataset using sklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcov\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_covtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\covtype.py\u001b[0m in \u001b[0;36mfetch_covtype\u001b[1;34m(data_home, download_if_missing, random_state, shuffle, return_X_y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloading %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mARCHIVE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0marchive_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fetch_remote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mARCHIVE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcovtype_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mXy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marchive_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# delete archive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\base.py\u001b[0m in \u001b[0;36m_fetch_remote\u001b[1;34m(remote, dirname)\u001b[0m\n\u001b[0;32m    912\u001b[0m     file_path = (remote.filename if dirname is None\n\u001b[0;32m    913\u001b[0m                  else join(dirname, remote.filename))\n\u001b[1;32m--> 914\u001b[1;33m     \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[0mchecksum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sha256\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mremote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchecksum\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mchecksum\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1010\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1012\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    872\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fetch dataset using sklearn\n",
    "cov = fetch_covtype()\n",
    "X = cov.data\n",
    "y = cov.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-merror:0.15670\n",
      "[1]\ttest-merror:0.15654\n",
      "[2]\ttest-merror:0.15652\n",
      "[3]\ttest-merror:0.15637\n",
      "[4]\ttest-merror:0.15632\n",
      "[5]\ttest-merror:0.15618\n",
      "[6]\ttest-merror:0.15621\n",
      "[7]\ttest-merror:0.15616\n",
      "[8]\ttest-merror:0.15609\n",
      "[9]\ttest-merror:0.15591\n",
      "[10]\ttest-merror:0.15585\n",
      "[11]\ttest-merror:0.15584\n",
      "[12]\ttest-merror:0.15584\n",
      "[13]\ttest-merror:0.15595\n",
      "[14]\ttest-merror:0.15588\n",
      "[15]\ttest-merror:0.15598\n",
      "[16]\ttest-merror:0.15599\n",
      "[17]\ttest-merror:0.15593\n",
      "[18]\ttest-merror:0.15591\n",
      "[19]\ttest-merror:0.15595\n",
      "[20]\ttest-merror:0.15597\n",
      "[21]\ttest-merror:0.15609\n",
      "[22]\ttest-merror:0.15631\n",
      "[23]\ttest-merror:0.15629\n",
      "[24]\ttest-merror:0.15628\n",
      "GPU Training Time: 15.797765016555786 seconds\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, train_size=0.75, random_state=42)\n",
    "num_round = 25 #3000\n",
    "\n",
    "# Leave most parameters as default\n",
    "param = {'objective': 'multi:softmax', # Specify multiclass classification\n",
    "         'num_class': 8, # Number of possible output classes\n",
    "         'tree_method': 'gpu_hist' # Use GPU accelerated algorithm\n",
    "         }\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "gpu_res = {} # Store accuracy result\n",
    "tmp = time.time()\n",
    "# Train model\n",
    "param['tree_method'] = 'gpu_hist'\n",
    "model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.1,\n",
    "       colsample_bytree=0.733, gamma=0.11, learning_rate=0.05, max_delta_step=0,\n",
    "       max_depth=25, min_child_weight=1, missing=None, n_estimators=25,\n",
    "       n_jobs=-1, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=1.7, reg_lambda=0.6, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1,gpu_id =1, max_bin = 16, tree_method = 'gpu_hist')\n",
    "model.fit(X_train,y_train)\n",
    "# xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=gpu_res) \n",
    "print(\"GPU Training Time: %s seconds\" % (str(time.time() - tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-merror:0.15670\n",
      "[1]\ttest-merror:0.15654\n",
      "[2]\ttest-merror:0.15652\n",
      "[3]\ttest-merror:0.15637\n",
      "[4]\ttest-merror:0.15632\n",
      "[5]\ttest-merror:0.15618\n",
      "[6]\ttest-merror:0.15621\n",
      "[7]\ttest-merror:0.15616\n",
      "[8]\ttest-merror:0.15609\n",
      "[9]\ttest-merror:0.15591\n",
      "[10]\ttest-merror:0.15585\n",
      "[11]\ttest-merror:0.15584\n",
      "[12]\ttest-merror:0.15584\n",
      "[13]\ttest-merror:0.15595\n",
      "[14]\ttest-merror:0.15588\n",
      "[15]\ttest-merror:0.15598\n",
      "[16]\ttest-merror:0.15599\n",
      "[17]\ttest-merror:0.15593\n",
      "[18]\ttest-merror:0.15591\n",
      "[19]\ttest-merror:0.15595\n",
      "[20]\ttest-merror:0.15597\n",
      "[21]\ttest-merror:0.15609\n",
      "[22]\ttest-merror:0.15631\n",
      "[23]\ttest-merror:0.15629\n",
      "[24]\ttest-merror:0.15628\n",
      "GPU Training Time: 14.73460841178894 seconds\n"
     ]
    }
   ],
   "source": [
    "gpu_res = {} # Store accuracy result\n",
    "tmp = time.time()\n",
    "# Train model\n",
    "param['tree_method'] = 'gpu_hist'\n",
    "xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=gpu_res)\n",
    "print(\"GPU Training Time: %s seconds\" % (str(time.time() - tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-merror:0.15668\n",
      "[1]\ttest-merror:0.15652\n",
      "[2]\ttest-merror:0.15651\n",
      "[3]\ttest-merror:0.15636\n",
      "[4]\ttest-merror:0.15631\n",
      "[5]\ttest-merror:0.15615\n",
      "[6]\ttest-merror:0.15609\n",
      "[7]\ttest-merror:0.15611\n",
      "[8]\ttest-merror:0.15606\n",
      "[9]\ttest-merror:0.15615\n",
      "[10]\ttest-merror:0.15596\n",
      "[11]\ttest-merror:0.15591\n",
      "[12]\ttest-merror:0.15574\n",
      "[13]\ttest-merror:0.15590\n",
      "[14]\ttest-merror:0.15587\n",
      "[15]\ttest-merror:0.15584\n",
      "[16]\ttest-merror:0.15587\n",
      "[17]\ttest-merror:0.15587\n",
      "[18]\ttest-merror:0.15594\n",
      "[19]\ttest-merror:0.15591\n",
      "[20]\ttest-merror:0.15590\n",
      "[21]\ttest-merror:0.15600\n",
      "[22]\ttest-merror:0.15594\n",
      "[23]\ttest-merror:0.15593\n",
      "[24]\ttest-merror:0.15598\n",
      "CPU Training Time: 166.61956787109375 seconds\n"
     ]
    }
   ],
   "source": [
    "# Repeat for CPU algorithm\n",
    "tmp = time.time()\n",
    "param['tree_method'] = 'hist'\n",
    "cpu_res = {}\n",
    "xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=cpu_res)\n",
    "print(\"CPU Training Time: %s seconds\" % (str(time.time() - tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
